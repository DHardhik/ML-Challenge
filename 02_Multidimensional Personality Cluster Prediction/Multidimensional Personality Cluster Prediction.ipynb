{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":120684,"databundleVersionId":14434865,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loading Data set","metadata":{}},{"cell_type":"code","source":"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np # For numerical operations and array handling\n\ntrain_ds_original = pd.read_csv('/kaggle/input/multidimensional-personality-cluster-prediction/train.csv')\ntest_ds_original = pd.read_csv('/kaggle/input/multidimensional-personality-cluster-prediction/test.csv')\n\n# want to see the data set shape\nprint(\"Given Train Data set Shape: \", train_ds_original.shape)\n\n# Get Dataset info\ntrain_ds_original.info()\nprint('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\ntest_ds_original.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# participant_id      -> index column\n# personality_cluster -> target column\nindex_column = 'participant_id'\ntarget_column = 'personality_cluster'\ntrain_ds = train_ds_original.drop(columns=[index_column, target_column])\n\n# Statistics of my Data set\ntrain_ds.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)\n1. Correlation Matrix\n2. Box plot\n3. Scattered Plot\n4. histogram","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(12, 8))\n\ncorr = train_ds.corr(numeric_only=True)   # ensures only numeric columns\n\nsns.heatmap(corr, annot=True, cmap=\"coolwarm\", linewidths=0.5)\n\nplt.title(\"Correlation Heatmap\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nnumeric_cols = train_ds_original.select_dtypes(include=['int64', 'float64']).columns\n\nplt.figure(figsize=(12, 6))\nplt.boxplot(train_ds_original[numeric_cols].values, labels=numeric_cols)\nplt.xticks(rotation=90)\nplt.title(\"Boxplot of All Numeric Features\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\nfeature = \"consistency_score\"   # üîÅ change to any feature you want\n\nclasses = train_ds_original[\"personality_cluster\"].unique()\n\ndata = [\n    train_ds_original[train_ds_original[\"personality_cluster\"] == cls][feature]\n    for cls in classes\n]\n\nplt.figure()\nplt.boxplot(\n    data,\n    labels=classes,\n    patch_artist=True,\n    boxprops=dict(facecolor='#01519a', alpha=0.15),\n    medianprops=dict(color='#01519a')\n)\n\nplt.xlabel(\"Personality Cluster\")\nplt.ylabel(feature)\nplt.title(f\"Boxplot of {feature} vs Personality Cluster\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport math\n\n# Select numeric features only\nnumeric_cols = train_ds_original.select_dtypes(include=['int64', 'float64']).columns\nnumeric_cols = [col for col in numeric_cols if col != \"participant_id\"]\n\nclusters = train_ds_original[\"personality_cluster\"].unique()\n\nn_features = len(numeric_cols)\ncols = 3                                  # 3 plots per row\nrows = math.ceil(n_features / cols)\n\nplt.figure(figsize=(cols*5, rows*4))\n\nfor i, feature in enumerate(numeric_cols, 1):\n\n    data = [\n        train_ds_original[train_ds_original[\"personality_cluster\"] == cls][feature]\n        for cls in clusters\n    ]\n\n    plt.subplot(rows, cols, i)\n    plt.boxplot(\n        data,\n        labels=clusters,\n        patch_artist=True,\n        boxprops=dict(facecolor=\"#01519a\", alpha=0.15),\n        medianprops=dict(color=\"#01519a\")\n    )\n\n    plt.title(feature)\n    plt.xlabel(\"Cluster\")\n    plt.ylabel(\"Value\")\n    plt.grid(True)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nx_feature = \"focus_intensity\"\ny_feature = \"consistency_score\"\n\nclusters = train_ds_original[\"personality_cluster\"].unique()\n\nplt.figure(figsize=(8, 6))\n\nfor cls in clusters:\n    subset = train_ds_original[train_ds_original[\"personality_cluster\"] == cls]\n    plt.scatter(\n        subset[x_feature],\n        subset[y_feature],\n        alpha=0.4,\n        label=cls,\n        color=\"#01519a\"\n    )\n\nplt.xlabel(x_feature)\nplt.ylabel(y_feature)\nplt.title(\"Scatter Plot of Focus Intensity vs Consistency Score\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport math\nfrom sklearn.preprocessing import LabelEncoder\n\n# Encode target for scatter plotting\nle_scatter = LabelEncoder()\ny_encoded_scatter = le_scatter.fit_transform(\n    train_ds_original[\"personality_cluster\"]\n)\n\n# Select numeric features (exclude ID)\nnumeric_cols = train_ds_original.select_dtypes(\n    include=['int64', 'float64']\n).columns\nnumeric_cols = [col for col in numeric_cols if col != \"participant_id\"]\n\nX_data = train_ds_original[numeric_cols]\n\nn_features = len(numeric_cols)\ncols = 3\nrows = math.ceil(n_features / cols)\n\nplt.figure(figsize=(cols * 5, rows * 4))\n\nfor i, feature in enumerate(numeric_cols, 1):\n    plt.subplot(rows, cols, i)\n\n    # Add small jitter to target for better visibility\n    jitter = np.random.normal(0, 0.05, size=len(y_encoded_scatter))\n\n    plt.scatter(\n        X_data[feature],\n        y_encoded_scatter + jitter,\n        alpha=0.4,\n        color=\"#01519a\"   # iiitbblue\n    )\n\n    plt.yticks(\n        np.arange(len(le_scatter.classes_)),\n        le_scatter.classes_\n    )\n\n    plt.xlabel(feature)\n    plt.ylabel(\"Personality Cluster\")\n    plt.title(f\"{feature} vs Target\")\n    plt.grid(True)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import gaussian_kde\n\n# Extract the feature\ndata = train_ds_original[\"consistency_score\"].dropna().values\n\n# Create KDE\nkde = gaussian_kde(data)\nx_vals = np.linspace(data.min(), data.max(), 300)\nkde_vals = kde(x_vals)\n\n# Plot\nplt.figure(figsize=(8, 5))\n\n# Histogram (iiitbblue)\nplt.hist(\n    data,\n    bins=30,\n    density=True,\n    color=\"#01519a\",      # iiitbblue\n    edgecolor=\"black\",\n    label=\"Histogram\"\n)\n\n# Density Curve (iiitbgray)\nplt.plot(\n    x_vals,\n    kde_vals,\n    color=\"#838280\",     # iiitbgray\n    linewidth=2,\n    label=\"Density\"\n)\n\nplt.xlabel(\"Consistency Score\")\nplt.ylabel(\"Density\")\nplt.title(\"Distribution of Consistency Score\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Training Arc","metadata":{}},{"cell_type":"markdown","source":"# 1. MLP","metadata":{}},{"cell_type":"code","source":"!pip uninstall -y protobuf\n!pip install protobuf==3.20.3\n!pip install --upgrade tensorflow\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\nimport numpy as np\n\ntarget_column = 'personality_cluster'\ny = train_ds_original[target_column]\n\nX_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n    train_ds, y, test_size=0.2, random_state=42, stratify=y\n)\n\nscaler = StandardScaler()\nX_train_split_scaled = scaler.fit_transform(X_train_split)\nX_val_split_scaled = scaler.transform(X_val_split)\n\nle = LabelEncoder()\ny_train_encoded = le.fit_transform(y_train_split)\ny_val_encoded = le.transform(y_val_split)\n\nnum_classes = len(np.unique(y_train_encoded))\n\ny_train_onehot = to_categorical(y_train_encoded, num_classes)\ny_val_onehot = to_categorical(y_val_encoded, num_classes)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nfrom sklearn.metrics import accuracy_score, classification_report\n\nmodel = Sequential([\n    Dense(256, activation='relu', input_shape=(X_train_split_scaled.shape[1],)),\n    BatchNormalization(),\n    Dropout(0.3),\n\n    Dense(128, activation='relu'),\n    Dropout(0.2),\n\n    Dense(num_classes, activation='softmax')\n])\n\nmodel.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n\nearly_stop = EarlyStopping(\n    monitor='val_loss',\n    patience=10,\n    restore_best_weights=True\n)\n\nhistory = model.fit(\n    X_train_split_scaled, y_train_onehot,\n    validation_data=(X_val_split_scaled, y_val_onehot),\n    epochs=150,\n    batch_size=32,\n    callbacks=[early_stop],\n    verbose=1\n)\n\n\n\nval_preds = model.predict(X_val_split_scaled)\nval_preds_labels = np.argmax(val_preds, axis=1)\n\nprint(\"MLP Validation Accuracy:\", accuracy_score(y_val_encoded, val_preds_labels))\nprint(classification_report(y_val_encoded, val_preds_labels))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure()\n\n# Train Accuracy - iiitbblue!15!white\nplt.plot(\n    history.history['accuracy'],\n    label='Train Accuracy',\n    color='#01519a',\n    linewidth=2\n)\n\n# Validation Accuracy - iiitbgray\nplt.plot(\n    history.history['val_accuracy'],\n    label='Validation Accuracy',\n    color='#838280',\n    linewidth=2\n)\n\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"MLP Training vs Validation Accuracy\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop target and index from test set\nX_test_final = test_ds_original.drop(columns=['participant_id'], errors='ignore')\n\n# Apply the SAME scaler used for training\nX_test_final_scaled = scaler.transform(X_test_final)\n\ntest_preds = model.predict(X_test_final_scaled)\ntest_preds_labels = np.argmax(test_preds, axis=1)\n\ntest_preds_clusters = le.inverse_transform(test_preds_labels)\n\nsubmission = pd.DataFrame({\n    \"participant_id\": test_ds_original['participant_id'],\n    \"personality_cluster\": test_preds_clusters\n})\n\nsubmission.to_csv(\"mlp_submission_26_14_44.csv\", index=False)\nprint(\"‚úÖ Submission file saved as mlp_submission_26_14_44.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y.value_counts(normalize=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Observed that there is so much unbalence in Target column","metadata":{}},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport numpy as np\n\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(y_train_encoded),\n    y=y_train_encoded\n)\nclass_weights = dict(enumerate(class_weights))\n\nmodel = Sequential([\n    Dense(512, activation='relu', input_shape=(X_train_split_scaled.shape[1],)),\n    BatchNormalization(),\n    Dropout(0.4),\n\n    Dense(256, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.3),\n\n    Dense(128, activation='relu'),\n    Dropout(0.2),\n\n    Dense(num_classes, activation='softmax')\n])\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0007),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nearly_stop = EarlyStopping(\n    monitor='val_loss',\n    patience=15,\n    restore_best_weights=True\n)\n\nhistory = model.fit(\n    X_train_split_scaled, y_train_onehot,\n    validation_data=(X_val_split_scaled, y_val_onehot),\n    epochs=150,\n    batch_size=32,\n    class_weight=class_weights,\n    callbacks=[early_stop],\n    verbose=1\n)\n\n# Drop target and index from test set\nX_test_final = test_ds_original.drop(columns=['participant_id'], errors='ignore')\n\n# Apply the SAME scaler used for training\nX_test_final_scaled = scaler.transform(X_test_final)\n\ntest_preds = model.predict(X_test_final_scaled)\ntest_preds_labels = np.argmax(test_preds, axis=1)\n\ntest_preds_clusters = le.inverse_transform(test_preds_labels)\n\nsubmission = pd.DataFrame({\n    \"participant_id\": test_ds_original['participant_id'],\n    \"personality_cluster\": test_preds_clusters\n})\n\nsubmission.to_csv(\"mlp_submission_26_14_57.csv\", index=False)\nprint(\"‚úÖ Submission file saved as mlp_submission_26_14_44.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Train Accuracy:\", history.history['accuracy'][-1])\nprint(\"Val Accuracy:\", history.history['val_accuracy'][-1])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. SVM","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Features and target\nX_train = train_ds_original.drop(columns=['participant_id', 'personality_cluster'])\ny_train = train_ds_original['personality_cluster']\n\nX_test = test_ds.drop(columns=['participant_id'])\n\n# Scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled  = scaler.transform(X_test)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'C': [0.1, 1, 10, 50, 100],\n    'gamma': [0.1, 0.01, 0.001, 'scale'],\n    'kernel': ['rbf']\n}\n\nsvm_grid = GridSearchCV(\n    SVC(probability=True),\n    param_grid,\n    cv=5,\n    n_jobs=-1,\n    verbose=2\n)\n\nsvm_grid.fit(X_train_scaled, y_train)\n\nbest_svm = svm_grid.best_estimator_\nprint(\"Best SVM Params:\", svm_grid.best_params_)\nprint(\"Best CV Accuracy:\", svm_grid.best_score_)\n\n\nsvm_test_preds = best_svm.predict(X_test_scaled)\n\nsubmission = pd.DataFrame({\n    \"participant_id\": test_ds[\"participant_id\"],\n    \"personality_cluster\": svm_test_preds\n})\n\nsubmission.to_csv(\"svm_submission.csv\", index=False)\nprint(\"‚úÖ SVM submission saved\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport numpy as np\n\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(y_train_encoded),\n    y=y_train_encoded\n)\nclass_weights = dict(enumerate(class_weights))\n\nmodel = Sequential([\n    Dense(128, activation='relu', input_shape=(X_train_split_scaled.shape[1],)),\n    BatchNormalization(),\n    Dropout(0.30),\n\n    Dense(64, activation='relu'),\n    Dropout(0.20),\n\n    Dense(num_classes, activation='softmax')\n])\n\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\nearly_stop = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True)\n\nhistory = model.fit(\n    X_train_split_scaled, y_train_onehot,\n    validation_data=(X_val_split_scaled, y_val_onehot),\n    epochs=150,\n    batch_size=32,\n    callbacks=[early_stop],\n    verbose=1\n)\ntest_preds = model.predict(X_test_scaled)\ntest_preds_numeric = np.argmax(test_preds, axis=1)\ntest_preds_clusters = le.inverse_transform(test_preds_numeric)\n\nsubmission = pd.DataFrame({\n    \"participant_id\": test_ds[\"participant_id\"],\n    \"personality_cluster\": test_preds_clusters\n})\n\nsubmission.to_csv(\"mlp_submission_v3.csv\", index=False)\nprint(\"‚úÖ MLP submission saved\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mlp_preds = pd.read_csv(\"mlp_submission_v3.csv\")[\"personality_cluster\"]\nsvm_preds = pd.read_csv(\"svm_submission.csv\")[\"personality_cluster\"]\n\nfinal_preds = []\n\nfor a, b in zip(mlp_preds, svm_preds):\n    if a == b:\n        final_preds.append(a)\n    else:\n        final_preds.append(a)  # tie-break ‚Üí trust MLP\n\nsubmission = pd.DataFrame({\n    \"participant_id\": test_ds[\"participant_id\"],\n    \"personality_cluster\": final_preds\n})\n\nsubmission.to_csv(\"svm_mlp_ensemble.csv\", index=False)\nprint(\"‚úÖ Ensemble submission saved\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n#             Applied PCA and then SVM\n#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nimport pandas as pd\n\n# Separate features and target\nX_train = train_ds_original.drop(columns=['participant_id', 'personality_cluster'])\ny_train = train_ds_original['personality_cluster']\n\nX_test = test_ds.drop(columns=['participant_id'])\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled  = scaler.transform(X_test)\n\npca = PCA(n_components=0.95, random_state=42)\n\nX_train_pca = pca.fit_transform(X_train_scaled)\nX_test_pca  = pca.transform(X_test_scaled)\n\nprint(\"Original features:\", X_train.shape[1])\nprint(\"Reduced features after PCA:\", X_train_pca.shape[1])\n\nparam_grid = {\n    'C': [0.5, 1, 5, 10, 50],\n    'gamma': ['scale', 0.01, 0.001],\n    'kernel': ['rbf']\n}\n\nsvm_grid = GridSearchCV(\n    SVC(probability=True),\n    param_grid,\n    cv=5,\n    n_jobs=-1,\n    verbose=2\n)\n\nsvm_grid.fit(X_train_pca, y_train)\n\nbest_svm = svm_grid.best_estimator_\n\nprint(\"Best SVM Params:\", svm_grid.best_params_)\nprint(\"Best CV Accuracy:\", svm_grid.best_score_)\n\nsvm_test_preds = best_svm.predict(X_test_pca)\n\nsubmission = pd.DataFrame({\n    \"participant_id\": test_ds[\"participant_id\"],\n    \"personality_cluster\": svm_test_preds\n})\n\nsubmission.to_csv(\"svm_pca_submission.csv\", index=False)\nprint(\"‚úÖ svm_pca_submission.csv saved\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n#             Using Grid Search CV\n#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'C': [0.5, 1, 2, 5, 10, 50],\n    'gamma': [0.05, 0.01, 0.005, 'scale'],\n    'kernel': ['rbf']\n}\n\nsvm_grid = GridSearchCV(\n    SVC(class_weight='balanced', probability=True),\n    param_grid,\n    cv=5,\n    n_jobs=-1,\n    verbose=2\n)\n\nsvm_grid.fit(X_train_scaled, y_train)\n\nbest_svm = svm_grid.best_estimator_\n\nprint(\"Best Params:\", svm_grid.best_params_)\nprint(\"Best CV Accuracy:\", svm_grid.best_score_)\n\nsvm_preds = best_svm.predict(X_test_scaled)\n\nsubmission = pd.DataFrame({\n    \"participant_id\": test_ds[\"participant_id\"],\n    \"personality_cluster\": svm_preds\n})\n\nsubmission.to_csv(\"svm_weighted_submission.csv\", index=False)\nprint(\"‚úÖ svm_weighted_submission.csv saved\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n#            Applied Poly\n#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n\nparam_grid_poly = {\n    'C': [1, 5, 10],\n    'degree': [2, 3, 4],\n    'gamma': ['scale'],\n    'kernel': ['poly']\n}\n\nsvm_poly = GridSearchCV(\n    SVC(class_weight='balanced'),\n    param_grid_poly,\n    cv=5,\n    n_jobs=-1,\n    verbose=2\n)\n\nsvm_poly.fit(X_train_scaled, y_train)\n\nbest_poly = svm_poly.best_estimator_\n\npoly_preds = best_poly.predict(X_test_scaled)\n\nsubmission = pd.DataFrame({\n    \"participant_id\": test_ds[\"participant_id\"],\n    \"personality_cluster\": poly_preds\n})\n\nsubmission.to_csv(\"svm_poly_submission.csv\", index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Ensemble SVM and MLP","metadata":{}},{"cell_type":"code","source":"\nsvm_probs = best_svm.predict_proba(X_test_scaled)\nmlp_probs = model.predict(X_test_scaled)\n\nfinal_probs = 0.5 * svm_probs + 0.5 * mlp_probs\nfinal_preds_numeric = np.argmax(final_probs, axis=1)\nfinal_preds = le.inverse_transform(final_preds_numeric)\n\nsubmission = pd.DataFrame({\n    \"participant_id\": test_ds[\"participant_id\"],\n    \"personality_cluster\": final_preds\n})\n\nsubmission.to_csv(\"svm_mlp_prob_ensemble.csv\", index=False)\nprint(\"‚úÖ svm_mlp_prob_ensemble.csv saved\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"UNderstood that we can't go ahead with SVM","metadata":{}},{"cell_type":"markdown","source":"# 4. Coming back to MLP","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nimport numpy as np\n\nX = train_ds_original.drop(columns=['participant_id', 'personality_cluster'])\ny = train_ds_original['personality_cluster']\n\nX_test = test_ds.drop(columns=['participant_id'])\ntest_ids = test_ds['participant_id']\n\n# Encode labels\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\n\n# Train/validation split\nX_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n)\n\n# Scale\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_split)\nX_val_scaled   = scaler.transform(X_val_split)\nX_test_scaled  = scaler.transform(X_test)\n\n# One-hot for TensorFlow\nnum_classes = len(np.unique(y_encoded))\ny_train_onehot = to_categorical(y_train_split, num_classes)\ny_val_onehot   = to_categorical(y_val_split, num_classes)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nmodel = Sequential([\n    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n    BatchNormalization(),\n    Dropout(0.30),\n\n    Dense(96, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.25),\n\n    Dense(64, activation='relu'),\n    Dropout(0.20),\n\n    Dense(num_classes, activation='softmax')\n])\n\nmodel.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nearly_stop = EarlyStopping(\n    monitor='val_loss',\n    patience=15,\n    restore_best_weights=True\n)\n\nlr_reduce = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=6,\n    min_lr=1e-5\n)\nhistory = model.fit(\n    X_train_scaled, y_train_onehot,\n    validation_data=(X_val_scaled, y_val_onehot),\n    epochs=300,\n    batch_size=32,\n    callbacks=[early_stop, lr_reduce],\n    verbose=1\n)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report\n\nval_preds = model.predict(X_val_scaled)\nval_preds_labels = np.argmax(val_preds, axis=1)\n\nprint(\"MLP Validation Accuracy:\", accuracy_score(y_val_split, val_preds_labels))\nprint(classification_report(y_val_split, val_preds_labels))\n\ntest_preds = model.predict(X_test_scaled)\ntest_preds_numeric = np.argmax(test_preds, axis=1)\ntest_preds_clusters = le.inverse_transform(test_preds_numeric)\n\nsubmission = pd.DataFrame({\n    \"participant_id\": test_ids,\n    \"personality_cluster\": test_preds_clusters\n})\n\nsubmission.to_csv(\"mlp_submission_v4.csv\", index=False)\nprint(\"‚úÖ nn_submission_final.csv saved\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Train Accuracy:\", history.history['accuracy'][-1])\nprint(\"Val Accuracy:\", history.history['val_accuracy'][-1])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nimport numpy as np\n\n\n# Full features and target\nX_full = train_ds_original.drop(columns=['participant_id', 'personality_cluster'])\ny_full = train_ds_original['personality_cluster']\n\n# Encode labels\nle = LabelEncoder()\ny_encoded = le.fit_transform(y_full)\n\n# Scale FULL training data\nscaler = StandardScaler()\nX_full_scaled = scaler.fit_transform(X_full)\n\nX_test = test_ds_original.drop(columns=['participant_id'])\n# Scale test data with SAME scaler\nX_test_scaled = scaler.transform(X_test)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n#           Feature Engineering\n#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n\n# X_full['focus_x_consistency'] = X_full['focus_intensity'] * X_full['consistency_score']\n# X_test['focus_x_consistency'] = X_test['focus_intensity'] * X_test['consistency_score']\n\n# X_full['support_x_focus'] = X_full['support_environment_score'] * X_full['focus_intensity']\n# X_test['support_x_focus'] = X_test['support_environment_score'] * X_test['focus_intensity']\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_model(input_dim, num_classes):\n    model = Sequential([\n        Dense(128, activation='relu', input_shape=(input_dim,)),\n        BatchNormalization(),\n        Dropout(0.3),\n\n        Dense(96, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.25),\n\n        Dense(64, activation='relu'),\n        Dropout(0.2),\n\n        Dense(num_classes, activation='softmax')\n    ])\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_classes = len(np.unique(y_encoded))\ntest_prob_sum = np.zeros((X_test_scaled.shape[0], num_classes))\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X_full_scaled, y_encoded)):\n    print(f\"\\n--- Fold {fold+1} ---\")\n\n    X_tr = X_full_scaled[train_idx]\n    X_va = X_full_scaled[val_idx]\n\n    y_tr = y_encoded[train_idx]\n    y_va = y_encoded[val_idx]\n\n    y_tr_oh = to_categorical(y_tr, num_classes)\n    y_va_oh = to_categorical(y_va, num_classes)\n\n    model = build_model(X_tr.shape[1], num_classes)\n\n    early_stop = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True)\n\n    model.fit(\n        X_tr, y_tr_oh,\n        validation_data=(X_va, y_va_oh),\n        epochs=200,\n        batch_size=32,\n        callbacks=[early_stop],\n        verbose=0\n    )\n\n    # Predict on Kaggle test set\n    test_fold_probs = model.predict(X_test_scaled)\n    test_prob_sum += test_fold_probs\n\nfinal_test_probs = test_prob_sum / 5\nfinal_test_numeric = np.argmax(final_test_probs, axis=1)\nfinal_test_clusters = le.inverse_transform(final_test_numeric)\n\nsubmission = pd.DataFrame({\n    \"participant_id\": test_ds[\"participant_id\"],\n    \"personality_cluster\": final_test_clusters\n})\n# greater than 1m. 1m 12s for v2 (without feature)\n# v3 with feature engineering\nsubmission.to_csv(\"nn_kfold_ensemble_v2.csv\", index=False)\nprint(\"‚úÖ nn_kfold_ensemble.csv saved\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Trying MULTI-RUN K-FOLD NEURAL NETWORK AVERAGING (FINAL STAGE)","metadata":{}},{"cell_type":"code","source":"def build_model(input_dim, num_classes, seed):\n    tf.keras.utils.set_random_seed(seed)\n\n    model = Sequential([\n        Dense(128, activation='relu', input_shape=(input_dim,)),\n        BatchNormalization(),\n        Dropout(0.30),\n\n        Dense(96, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.25),\n\n        Dense(64, activation='relu'),\n        Dropout(0.20),\n\n        Dense(num_classes, activation='softmax')\n    ])\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\nRUNS = 5   # number of independent NN runs\nFOLDS = 5 # k-fold inside each run\n\nnum_classes = len(np.unique(y_encoded))\nfinal_test_prob_sum = np.zeros((X_test_scaled.shape[0], num_classes))\n\nfor run in range(RUNS):\n    print(f\"\\n================ RUN {run+1} ================\")\n\n    skf = StratifiedKFold(\n        n_splits=FOLDS,\n        shuffle=True,\n        random_state=42 + run\n    )\n\n    test_prob_sum = np.zeros((X_test_scaled.shape[0], num_classes))\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X_full_scaled, y_encoded)):\n        print(f\"  Fold {fold+1}\")\n\n        X_tr = X_full_scaled[train_idx]\n        X_va = X_full_scaled[val_idx]\n\n        y_tr = y_encoded[train_idx]\n        y_va = y_encoded[val_idx]\n\n        y_tr_oh = to_categorical(y_tr, num_classes)\n        y_va_oh = to_categorical(y_va, num_classes)\n\n        model = build_model(X_tr.shape[1], num_classes, seed=100 + run*10 + fold)\n\n        early_stop = EarlyStopping(\n            monitor='val_loss',\n            patience=12,\n            restore_best_weights=True\n        )\n\n        model.fit(\n            X_tr, y_tr_oh,\n            validation_data=(X_va, y_va_oh),\n            epochs=200,\n            batch_size=32,\n            callbacks=[early_stop],\n            verbose=0\n        )\n\n        fold_test_probs = model.predict(X_test_scaled)\n        test_prob_sum += fold_test_probs\n\n    # Average this run‚Äôs 5 folds\n    test_prob_sum /= FOLDS\n    final_test_prob_sum += test_prob_sum\n\nfinal_test_probs = final_test_prob_sum / RUNS\nfinal_test_numeric = np.argmax(final_test_probs, axis=1)\nfinal_test_clusters = le.inverse_transform(final_test_numeric)\n\nsubmission = pd.DataFrame({\n    \"participant_id\": test_ids,\n    \"personality_cluster\": final_test_clusters\n})\n# 5m 26s\nsubmission.to_csv(\"nn_multirun_ensemble.csv\", index=False)\nprint(\"‚úÖ nn_multirun_ensemble.csv saved\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Top 5 Model training Part 1","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import f1_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# ============================================================\n# 1. PREPROCESSING STRATEGIES\n# ============================================================\n\ndef get_preprocessor(strategy, categorical_cols, numerical_cols, binary_cols):\n\n    if strategy == \"standard\":\n        scaler = StandardScaler()\n        encode = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n\n    elif strategy == \"minmax\":\n        scaler = MinMaxScaler()\n        encode = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n\n    elif strategy == \"robust\":\n        scaler = RobustScaler()\n        encode = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n\n    elif strategy == \"labelencode_only\":\n        return \"labelencode_only\"   # special handling\n\n    else:  # none\n        return \"none\"\n\n    return ColumnTransformer(\n        transformers=[\n            (\"num\", scaler, numerical_cols),\n            (\"cat\", encode, categorical_cols),\n            (\"bin\", \"passthrough\", binary_cols)\n        ]\n    )\n\n\n# ============================================================\n# 2. MAIN FUNCTION ‚Äì SVM + MLP TESTING (WITH TOP 5 MODELS)\n# ============================================================\n\ndef evaluate_models(train_df, test_df):\n\n    # ---------------------------------------------\n    # Step A: Split X and y\n    # ---------------------------------------------\n    X = train_df.drop([\"participant_id\", \"personality_cluster\"], axis=1)\n    y = train_df[\"personality_cluster\"]\n\n    le_target = LabelEncoder()\n    y_encoded = le_target.fit_transform(y)\n\n    # Categorical & numerical groups\n    categorical_cols = [\"cultural_background\"]\n\n    numerical_cols = [\n        \"age_group\", \"upbringing_influence\", \"focus_intensity\",\n        \"consistency_score\", \"support_environment_score\"\n    ]\n\n    binary_cols = [\n        \"identity_code\", \"external_guidance_usage\", \"hobby_engagement_level\",\n        \"physical_activity_index\", \"creative_expression_index\", \"altruism_score\"\n    ]\n\n    # Split\n    X_train, X_val, y_train, y_val = train_test_split(\n        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n    )\n\n    preprocessing_strategies = [\"standard\", \"minmax\", \"robust\", \"labelencode_only\", \"none\"]\n    svm_C_values = [1, 10, 50]\n\n    # Expanded MLP sizes\n    mlp_sizes = [\n        (64,),\n        (128,),\n        (256,),\n        (64, 32),\n        (128, 64),\n        (256, 128),\n        (128, 128),\n        (256, 128, 64)\n    ]\n\n    # Track ALL models\n    model_results = []\n\n    # ============================================================\n    # 3. LOOP OVER PREPROCESSORS + SVM + MLP\n    # ============================================================\n\n    for strategy in preprocessing_strategies:\n\n        print(\"\\n\\n==============================================\")\n        print(f\" Testing Preprocessing Strategy: {strategy} \")\n        print(\"==============================================\")\n\n        # --------------------------------------\n        # Prepare preprocessing\n        # --------------------------------------\n        if strategy == \"labelencode_only\":\n            label_encoders = {}\n            X_train_copy = X_train.copy()\n            X_val_copy = X_val.copy()\n\n            for col in categorical_cols:\n                le = LabelEncoder()\n                X_train_copy[col] = le.fit_transform(X_train_copy[col])\n                X_val_copy[col] = le.transform(X_val_copy[col])\n                label_encoders[col] = le\n\n            Xt = X_train_copy\n            Xv = X_val_copy\n\n        elif strategy == \"none\":\n            Xt = X_train.copy()\n            Xv = X_val.copy()\n\n        else:\n            preprocessor = get_preprocessor(strategy, categorical_cols, numerical_cols, binary_cols)\n            Xt = preprocessor.fit_transform(X_train)\n            Xv = preprocessor.transform(X_val)\n\n        # ============================================================\n        # SVM MODELS\n        # ============================================================\n\n        for C in svm_C_values:\n            svm = SVC(C=C, kernel=\"rbf\", gamma=\"scale\")\n            svm.fit(Xt, y_train)\n\n            preds = svm.predict(Xv)\n            f1 = f1_score(y_val, preds, average=\"macro\")\n            print(f\"SVM (strategy={strategy}, C={C}) ‚Üí F1={f1:.4f}\")\n\n            model_results.append({\n                \"name\": f\"SVM_C{C}_{strategy}\",\n                \"f1\": f1,\n                \"model\": svm,\n                \"preprocessor\": \"none\" if strategy==\"none\" else (\n                    \"labelencode_only\" if strategy==\"labelencode_only\" else preprocessor\n                )\n            })\n\n        # ============================================================\n        # MLP MODELS\n        # ============================================================\n\n        for layers in mlp_sizes:\n            mlp = MLPClassifier(\n                hidden_layer_sizes=layers,\n                max_iter=1200,\n                activation=\"relu\",\n                solver=\"adam\",\n                learning_rate_init=0.001\n            )\n\n            mlp.fit(Xt, y_train)\n            preds = mlp.predict(Xv)\n            f1 = f1_score(y_val, preds, average=\"macro\")\n\n            print(f\"MLP (strategy={strategy}, layers={layers}) ‚Üí F1={f1:.4f}\")\n\n            model_results.append({\n                \"name\": f\"MLP_{layers}_{strategy}\",\n                \"f1\": f1,\n                \"model\": mlp,\n                \"preprocessor\": \"none\" if strategy==\"none\" else (\n                    \"labelencode_only\" if strategy==\"labelencode_only\" else preprocessor\n                )\n            })\n\n    # ============================================================\n    # 4. SELECT TOP 5 MODELS\n    # ============================================================\n\n    model_results = sorted(model_results, key=lambda x: x[\"f1\"], reverse=True)\n    top5 = model_results[:5]\n\n    print(\"\\n========== TOP 5 MODELS ==========\")\n    for i, m in enumerate(top5, 1):\n        print(f\"{i}. {m['name']} ‚Üí F1 = {m['f1']:.4f}\")\n\n    # ============================================================\n    # 5. GENERATE CSV FOR TOP 5 MODELS\n    # ============================================================\n\n    X_test = test_df.drop([\"participant_id\"], axis=1)\n    test_ids = test_df[\"participant_id\"]\n\n    for rank, m in enumerate(top5, 1):\n        model = m[\"model\"]\n        preproc = m[\"preprocessor\"]\n\n        # Apply correct preprocessing\n        if preproc == \"none\":\n            Xtst = X_test.copy()\n\n        elif preproc == \"labelencode_only\":\n            Xtst = X_test.copy()\n            for col in categorical_cols:\n                Xtst[col] = label_encoders[col].transform(X_test[col])\n\n        else:\n            Xtst = preproc.transform(X_test)\n\n        preds = model.predict(Xtst)\n        preds_labels = le_target.inverse_transform(preds)\n\n        filename = f\"top{rank}_{m['name']}.csv\"\n        output = pd.DataFrame({\n            \"participant_id\": test_ids,\n            \"personality_cluster\": preds_labels\n        })\n        output.to_csv(filename, index=False)\n        print(f\"Saved {filename}\")\n\n    return top5\n\n\nimport pandas as pd\n\ntrain_df = pd.read_csv(\"/content/train.csv\")\ntest_df = pd.read_csv(\"/content/test.csv\")\n\nbest_model_name, best_f1 = evaluate_models(train_df, test_df)\n\nprint(\"Best model:\", best_model_name)\nprint(\"Best F1 score:\", best_f1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Top 5 Best Models - Best Kaggle Score","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\nimport numpy as np\n\nnum_classes = len(np.unique(y_encoded))\ny_full_onehot = to_categorical(y_encoded, num_classes)\n\nprint(\"num_classes:\", num_classes)\nprint(\"y_full_onehot shape:\", y_full_onehot.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n    X_full_scaled,\n    y_encoded,\n    test_size=0.2,\n    random_state=42,\n    stratify=y_encoded\n)\n\n# One-hot for NN\ny_train_onehot = to_categorical(y_train_split, num_classes)\ny_val_onehot   = to_categorical(y_val_split, num_classes)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\n\nmodel = Sequential([\n    Dense(256, activation='relu', input_shape=(X_train_split.shape[1],)),\n    BatchNormalization(),\n    Dropout(0.35),\n\n    Dense(128, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.25),\n\n    Dense(64, activation='relu'),\n    Dropout(0.20),\n\n    Dense(num_classes, activation='softmax')\n])\n\nmodel.compile(\n    optimizer=Adam(learning_rate=0.0008),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nearly_stop = EarlyStopping(\n    monitor='val_loss',\n    patience=12,\n    restore_best_weights=True\n)\n\nhistory = model.fit(\n    X_train_split,\n    y_train_onehot,\n    validation_data=(X_val_split, y_val_onehot),\n    epochs=200,\n    batch_size=64,\n    callbacks=[early_stop],\n    verbose=1\n)\n\ntest_probs = model.predict(X_test_scaled)\ntest_preds_numeric = np.argmax(test_probs, axis=1)\n\n# Convert back to Cluster labels\ntest_preds_clusters = le.inverse_transform(test_preds_numeric)\n\nsubmission = pd.DataFrame({\n    \"participant_id\": test_ds_original[\"participant_id\"],\n    \"personality_cluster\": test_preds_clusters\n})\n\nsubmission.to_csv(\"nn_submission_v1.csv\", index=False)\nprint(\"‚úÖ submission_nn_final.csv saved\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import f1_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# ============================================================\n# 1. PREPROCESSING STRATEGIES\n# ============================================================\n\ndef get_preprocessor(strategy, categorical_cols, numerical_cols, binary_cols):\n    \"\"\"\n    Return a fitted or to-be-fitted ColumnTransformer for a given strategy.\n    For 'labelencode_only' and 'none', this function returns a string marker.\n    \"\"\"\n\n    if strategy == \"standard\":\n        scaler = StandardScaler()\n        encode = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n\n    elif strategy == \"minmax\":\n        scaler = MinMaxScaler()\n        encode = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n\n    elif strategy == \"robust\":\n        scaler = RobustScaler()\n        encode = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n\n    elif strategy == \"labelencode_only\":\n        return \"labelencode_only\"\n\n    else:  # \"none\"\n        return \"none\"\n\n    return ColumnTransformer(\n        transformers=[\n            (\"num\", scaler, numerical_cols),\n            (\"cat\", encode, categorical_cols),\n            (\"bin\", \"passthrough\", binary_cols)\n        ]\n    )\n\n\n# ============================================================\n# 2. MAIN FUNCTION ‚Äì SVM + MLP TESTING\n# ============================================================\n\ndef evaluate_models(train_df, test_df):\n\n    # ---------------------------------------------\n    # Step A: Split X and y\n    # ---------------------------------------------\n    X = train_df.drop([\"participant_id\", \"personality_cluster\"], axis=1)\n    y = train_df[\"personality_cluster\"]\n\n    le_target = LabelEncoder()\n    y_encoded = le_target.fit_transform(y)\n\n    # Categorical & numerical groups\n    categorical_cols = [\"cultural_background\"]\n\n    numerical_cols = [\n        \"age_group\", \"upbringing_influence\", \"focus_intensity\",\n        \"consistency_score\", \"support_environment_score\"\n    ]\n\n    binary_cols = [\n        \"identity_code\", \"external_guidance_usage\", \"hobby_engagement_level\",\n        \"physical_activity_index\", \"creative_expression_index\", \"altruism_score\"\n    ]\n\n    # Split train / val\n    X_train, X_val, y_train, y_val = train_test_split(\n        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n    )\n\n    # What we‚Äôll try\n    preprocessing_strategies = [\"standard\", \"minmax\", \"robust\", \"labelencode_only\", \"none\"]\n\n    svm_C_values = [0.1, 1, 10, 50, 100]\n    svm_gamma_values = [0.01, 0.05, 0.1, \"scale\"]\n\n    mlp_sizes = [\n        (64,),\n        (128,),\n        (256,),\n        (128, 64),\n        (256, 128),\n        (256, 128, 64)\n    ]\n\n    alphas = [1e-5, 1e-4, 1e-3]\n    learning_rates = [0.001, 0.0005]\n\n    model_results = []\n\n    # ============================================================\n    # 3. LOOP OVER PREPROCESSORS + SVM + MLP\n    # ============================================================\n\n    for strategy in preprocessing_strategies:\n\n        print(\"\\n==============================================\")\n        print(f\" Testing Preprocessing Strategy: {strategy} \")\n        print(\"==============================================\")\n\n        label_encoders = None\n        preprocessor = get_preprocessor(strategy, categorical_cols, numerical_cols, binary_cols)\n\n        # ----- Preprocessing -----\n        if preprocessor == \"labelencode_only\":\n            label_encoders = {}\n            X_train_copy = X_train.copy()\n            X_val_copy = X_val.copy()\n\n            for col in categorical_cols:\n                le = LabelEncoder()\n                X_train_copy[col] = le.fit_transform(X_train_copy[col])\n                X_val_copy[col] = le.transform(X_val_copy[col])\n                label_encoders[col] = le\n\n            Xt, Xv = X_train_copy, X_val_copy\n            fitted_preprocessor = \"labelencode_only\"\n\n        elif preprocessor == \"none\":\n            Xt, Xv = X_train.copy(), X_val.copy()\n            fitted_preprocessor = \"none\"\n\n        else:\n            # Fit transformer on train, apply to train/val\n            fitted_preprocessor = preprocessor.fit(X_train)\n            Xt = fitted_preprocessor.transform(X_train)\n            Xv = fitted_preprocessor.transform(X_val)\n\n        # ============================================================\n        # SVM MODELS\n        # ============================================================\n\n        for C in svm_C_values:\n            for gamma in svm_gamma_values:\n\n                svm = SVC(\n                    C=C,\n                    gamma=gamma,\n                    kernel=\"rbf\",\n                    class_weight=\"balanced\",\n                    probability=True\n                )\n\n                svm.fit(Xt, y_train)\n                preds = svm.predict(Xv)\n\n                f1 = f1_score(y_val, preds, average=\"macro\")\n\n                print(f\"SVM (strategy={strategy}, C={C}, gamma={gamma}) ‚Üí F1={f1:.4f}\")\n\n                model_results.append({\n                    \"name\": f\"SVM_C{C}_G{gamma}_{strategy}\",\n                    \"f1\": f1,\n                    \"model\": svm,\n                    \"preprocessor\": fitted_preprocessor,\n                    \"label_encoders\": label_encoders\n                })\n\n        # ============================================================\n        # MLP MODELS\n        # ============================================================\n\n        for layers in mlp_sizes:\n            for alpha in alphas:\n                for lr in learning_rates:\n\n                    mlp = MLPClassifier(\n                        hidden_layer_sizes=layers,\n                        max_iter=1500,\n                        activation=\"relu\",\n                        solver=\"adam\",\n                        learning_rate_init=lr,\n                        alpha=alpha,\n                        early_stopping=True,\n                        n_iter_no_change=20,\n                        random_state=42\n                    )\n\n                    mlp.fit(Xt, y_train)\n                    preds = mlp.predict(Xv)\n\n                    f1 = f1_score(y_val, preds, average=\"macro\")\n\n                    print(\n                        f\"MLP (strategy={strategy}, layers={layers}, \"\n                        f\"alpha={alpha}, lr={lr}) ‚Üí F1={f1:.4f}\"\n                    )\n\n                    model_results.append({\n                        \"name\": f\"MLP_{layers}_A{alpha}_LR{lr}_{strategy}\",\n                        \"f1\": f1,\n                        \"model\": mlp,\n                        \"preprocessor\": fitted_preprocessor,\n                        \"label_encoders\": label_encoders\n                    })\n\n    # ============================================================\n    # 4. SELECT TOP 5 MODELS\n    # ============================================================\n\n    model_results = sorted(model_results, key=lambda x: x[\"f1\"], reverse=True)\n    top5 = model_results[:5]\n\n    print(\"\\n========== TOP 5 MODELS ==========\")\n    for i, m in enumerate(top5, 1):\n        print(f\"{i}. {m['name']} ‚Üí F1 = {m['f1']:.4f}\")\n\n    # ============================================================\n    # 5. GENERATE CSV FOR TOP 5 MODELS\n    # ============================================================\n\n    X_test = test_df.drop([\"participant_id\"], axis=1)\n    test_ids = test_df[\"participant_id\"]\n\n    for rank, m in enumerate(top5, 1):\n\n        model = m[\"model\"]\n        fitted_preprocessor = m[\"preprocessor\"]\n        label_encs = m[\"label_encoders\"]\n\n        # Apply same preprocessing to test\n        if fitted_preprocessor == \"none\":\n            Xtst = X_test.copy()\n\n        elif fitted_preprocessor == \"labelencode_only\":\n            Xtst = X_test.copy()\n            for col in categorical_cols:\n                Xtst[col] = label_encs[col].transform(X_test[col])\n\n        else:\n            Xtst = fitted_preprocessor.transform(X_test)\n\n        preds = model.predict(Xtst)\n        preds_labels = le_target.inverse_transform(preds)\n\n        filename = f\"mlp_submission_v{rank+9}.csv\"\n        output = pd.DataFrame({\n            \"participant_id\": test_ids,\n            \"personality_cluster\": preds_labels\n        })\n\n        output.to_csv(filename, index=False)\n        print(f\"‚úÖ Saved {filename}\")\n\n    return top5\n\n\n# ============================================================\n# 3. RUN PIPELINE\n# ============================================================\n\ntrain_df = pd.read_csv(\"/content/train.csv\")\ntest_df  = pd.read_csv(\"/content/test.csv\")\n\ntop5 = evaluate_models(train_df, test_df)\nprint(\"‚úÖ Pipeline completed successfully.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7. Top 5 Best Models - applied Feature Engineering\ntried to improve previous code score but failed","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import f1_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# ============================================================\n# 1. PREPROCESSING STRATEGIES\n# ============================================================\n\ndef get_preprocessor(strategy, categorical_cols, numerical_cols, binary_cols):\n    \"\"\"\n    Return a fitted or to-be-fitted ColumnTransformer for a given strategy.\n    For 'labelencode_only' and 'none', this function returns a string marker.\n    \"\"\"\n\n    if strategy == \"standard\":\n        scaler = StandardScaler()\n        encode = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n\n    elif strategy == \"minmax\":\n        scaler = MinMaxScaler()\n        encode = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n\n    elif strategy == \"robust\":\n        scaler = RobustScaler()\n        encode = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n\n    elif strategy == \"labelencode_only\":\n        return \"labelencode_only\"\n\n    else:  # \"none\"\n        return \"none\"\n\n    return ColumnTransformer(\n        transformers=[\n            (\"num\", scaler, numerical_cols),\n            (\"cat\", encode, categorical_cols),\n            (\"bin\", \"passthrough\", binary_cols)\n        ]\n    )\n\n\n# ============================================================\n# 2. MAIN FUNCTION ‚Äì SVM + MLP TESTING\n# ============================================================\n\ndef evaluate_models(train_df, test_df):\n\n    # ---------------------------------------------\n    # Step A: Split X and y\n    # ---------------------------------------------\n    X = train_df.drop([\"participant_id\", \"personality_cluster\"], axis=1)\n    y = train_df[\"personality_cluster\"]\n\n    le_target = LabelEncoder()\n    y_encoded = le_target.fit_transform(y)\n\n    # Categorical & numerical groups\n    categorical_cols = [\"cultural_background\"]\n\n    numerical_cols = [\n        \"age_group\", \"upbringing_influence\", \"focus_intensity\",\n        \"consistency_score\", \"support_environment_score\"\n    ]\n\n    binary_cols = [\n        \"identity_code\", \"external_guidance_usage\", \"hobby_engagement_level\",\n        \"physical_activity_index\", \"creative_expression_index\", \"altruism_score\"\n    ]\n\n    # Split train / val\n    X_train, X_val, y_train, y_val = train_test_split(\n        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n    )\n\n    # What we‚Äôll try\n    preprocessing_strategies = [\"standard\", \"minmax\", \"robust\", \"labelencode_only\", \"none\"]\n\n    svm_C_values = [0.1, 1, 10, 50, 100]\n    svm_gamma_values = [0.01, 0.05, 0.1, \"scale\"]\n\n    mlp_sizes = [\n        (64,),\n        (128,),\n        (256,),\n        (128, 64),\n        (256, 128),\n        (256, 128, 64)\n    ]\n\n    alphas = [1e-5, 1e-4, 1e-3]\n    learning_rates = [0.001, 0.0005]\n\n    model_results = []\n\n    # ============================================================\n    # 3. LOOP OVER PREPROCESSORS + SVM + MLP\n    # ============================================================\n\n    for strategy in preprocessing_strategies:\n\n        print(\"\\n==============================================\")\n        print(f\" Testing Preprocessing Strategy: {strategy} \")\n        print(\"==============================================\")\n\n        label_encoders = None\n        preprocessor = get_preprocessor(strategy, categorical_cols, numerical_cols, binary_cols)\n\n        # ----- Preprocessing -----\n        if preprocessor == \"labelencode_only\":\n            label_encoders = {}\n            X_train_copy = X_train.copy()\n            X_val_copy = X_val.copy()\n\n            for col in categorical_cols:\n                le = LabelEncoder()\n                X_train_copy[col] = le.fit_transform(X_train_copy[col])\n                X_val_copy[col] = le.transform(X_val_copy[col])\n                label_encoders[col] = le\n\n            Xt, Xv = X_train_copy, X_val_copy\n            fitted_preprocessor = \"labelencode_only\"\n\n        elif preprocessor == \"none\":\n            Xt, Xv = X_train.copy(), X_val.copy()\n            fitted_preprocessor = \"none\"\n\n        else:\n            # Fit transformer on train, apply to train/val\n            fitted_preprocessor = preprocessor.fit(X_train)\n            Xt = fitted_preprocessor.transform(X_train)\n            Xv = fitted_preprocessor.transform(X_val)\n\n        # ============================================================\n        # SVM MODELS\n        # ============================================================\n\n        for C in svm_C_values:\n            for gamma in svm_gamma_values:\n\n                svm = SVC(\n                    C=C,\n                    gamma=gamma,\n                    kernel=\"rbf\",\n                    class_weight=\"balanced\",\n                    probability=True\n                )\n\n                svm.fit(Xt, y_train)\n                preds = svm.predict(Xv)\n\n                f1 = f1_score(y_val, preds, average=\"macro\")\n\n                print(f\"SVM (strategy={strategy}, C={C}, gamma={gamma}) ‚Üí F1={f1:.4f}\")\n\n                model_results.append({\n                    \"name\": f\"SVM_C{C}_G{gamma}_{strategy}\",\n                    \"f1\": f1,\n                    \"model\": svm,\n                    \"preprocessor\": fitted_preprocessor,\n                    \"label_encoders\": label_encoders\n                })\n\n        # ============================================================\n        # MLP MODELS\n        # ============================================================\n\n        for layers in mlp_sizes:\n            for alpha in alphas:\n                for lr in learning_rates:\n\n                    mlp = MLPClassifier(\n                        hidden_layer_sizes=layers,\n                        max_iter=1500,\n                        activation=\"relu\",\n                        solver=\"adam\",\n                        learning_rate_init=lr,\n                        alpha=alpha,\n                        early_stopping=True,\n                        n_iter_no_change=20,\n                        random_state=42\n                    )\n\n                    mlp.fit(Xt, y_train)\n                    preds = mlp.predict(Xv)\n\n                    f1 = f1_score(y_val, preds, average=\"macro\")\n\n                    print(\n                        f\"MLP (strategy={strategy}, layers={layers}, \"\n                        f\"alpha={alpha}, lr={lr}) ‚Üí F1={f1:.4f}\"\n                    )\n\n                    model_results.append({\n                        \"name\": f\"MLP_{layers}_A{alpha}_LR{lr}_{strategy}\",\n                        \"f1\": f1,\n                        \"model\": mlp,\n                        \"preprocessor\": fitted_preprocessor,\n                        \"label_encoders\": label_encoders\n                    })\n\n    # ============================================================\n    # 4. SELECT TOP 5 MODELS\n    # ============================================================\n\n    model_results = sorted(model_results, key=lambda x: x[\"f1\"], reverse=True)\n    top5 = model_results[:5]\n\n    print(\"\\n========== TOP 5 MODELS ==========\")\n    for i, m in enumerate(top5, 1):\n        print(f\"{i}. {m['name']} ‚Üí F1 = {m['f1']:.4f}\")\n\n    # ============================================================\n    # 5. GENERATE CSV FOR TOP 5 MODELS\n    # ============================================================\n\n    X_test = test_df.drop([\"participant_id\"], axis=1)\n    test_ids = test_df[\"participant_id\"]\n\n    for rank, m in enumerate(top5, 1):\n\n        model = m[\"model\"]\n        fitted_preprocessor = m[\"preprocessor\"]\n        label_encs = m[\"label_encoders\"]\n\n        # Apply same preprocessing to test\n        if fitted_preprocessor == \"none\":\n            Xtst = X_test.copy()\n\n        elif fitted_preprocessor == \"labelencode_only\":\n            Xtst = X_test.copy()\n            for col in categorical_cols:\n                Xtst[col] = label_encs[col].transform(X_test[col])\n\n        else:\n            Xtst = fitted_preprocessor.transform(X_test)\n\n        preds = model.predict(Xtst)\n        preds_labels = le_target.inverse_transform(preds)\n\n        filename = f\"mlp_submission_v{rank+14}.csv\"\n        output = pd.DataFrame({\n            \"participant_id\": test_ids,\n            \"personality_cluster\": preds_labels\n        })\n\n        output.to_csv(filename, index=False)\n        print(f\"‚úÖ Saved {filename}\")\n\n    return top5\n\n\n# ============================================================\n# 3. RUN PIPELINE\n# ============================================================\n\ntrain_df = pd.read_csv(\"/content/train.csv\")\ntest_df  = pd.read_csv(\"/content/test.csv\")\ndef add_safe_fe(df):\n\n    # small interaction features (scaled naturally)\n    df[\"focus_consistency\"] = df[\"focus_intensity\"] * df[\"consistency_score\"]\n\n    df[\"support_guidance\"] = (\n        df[\"support_environment_score\"] + df[\"external_guidance_usage\"]\n    ) / 2  # NOT multiplication\n\n    df[\"creative_hobby_mean\"] = (\n        df[\"creative_expression_index\"] + df[\"hobby_engagement_level\"]\n    ) / 2\n\n    df[\"activity_strength\"] = (\n        df[\"physical_activity_index\"]\n        + df[\"hobby_engagement_level\"]\n        + df[\"creative_expression_index\"]\n    ) / 3\n\n    df[\"stability_mean\"] = (\n        df[\"consistency_score\"] + df[\"support_environment_score\"]\n    ) / 2\n\n    df[\"guidance_ratio\"] = df[\"external_guidance_usage\"] / (\n        1 + df[\"support_environment_score\"]\n    )\n\n    return df\n\ntrain_df=add_safe_fe(train_df)\ntest_df=add_safe_fe(test_df)\n\ntop5 = evaluate_models(train_df, test_df)\nprint(\"‚úÖ Pipeline completed successfully.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 8. KNN","metadata":{}},{"cell_type":"code","source":"X_train = train_ds.drop([\"participant_id\", \"personality_cluster\"], axis=1)\ny_train = train_ds[\"personality_cluster\"]\n\nX_test = test_ds.drop([\"participant_id\"], axis=1)\ntest_ids = test_ds[\"participant_id\"]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------\n# 3. Scaling\n# ---------------------------\nscaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# ---------------------------\n# 4. KNN Model\n# ---------------------------\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train_scaled, y_train)\n\n# ---------------------------\n# 5. Predict on Test Dataset\n# ---------------------------\npreds = knn.predict(X_test_scaled)\n\n# ---------------------------\n# 6. Save Submission CSV\n# ---------------------------\nsubmission = pd.DataFrame({\n    \"participant_id\": test_ids,\n    \"personality_cluster\": preds\n})\n\nsubmission.to_csv(\"knn_submission_21_1923.csv\", index=False)\nprint(\"File saved as knn_submission_21_1923.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n)\n\nscaler = StandardScaler()\nX_train_split_scaled = scaler.fit_transform(X_train_split)\nX_val_split_scaled = scaler.transform(X_val_split)\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train_split_scaled, y_train_split)\n\nfrom sklearn.metrics import accuracy_score\n\nval_pred = knn.predict(X_val_split_scaled)\nprint(\"Validation Accuracy:\", accuracy_score(y_val_split, val_pred))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
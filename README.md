# ğŸ§  AIT 511 â€” Machine Learning | IIIT Bangalore  
### ğŸ“‚ ML Challenge Repository â€” *Project Katakam*

This repository contains **three Kaggle Machine Learning challenges** completed as part of **AIT 511 (Machine Learning)** at **IIIT Bangalore**.  
Each project focuses on a different ML problem: **binary classification, multiclass classification, and regression**.
---

## ğŸ“‚ Repository Structure

---
# ğŸ¥ Medical Equipments Cost Prediction Challenge
This project predicts **transportation costs** for delivering medical equipment to hospitals using machine learning techniques.  
It was developed as part of the **Kaggle Challenge: [Medical Equipments Cost Prediction](https://www.kaggle.com/competitions/Medical-Equipments-Cost-Prediction-Challenge)**.

## ğŸš€ Project Overview

The goal is to **predict the transport cost** of medical equipment deliveries using data such as equipment size, supplier reliability, shipping method, and order details.  
We applied several regression techniques and compared their performance using **RÂ²**, **MAE**, and **Kaggle leaderboard scores**.

---

## ğŸ§® Machine Learning Workflow

1. **Data Preprocessing**
   - Handled missing values with `SimpleImputer`
   - Scaled numerical features with `RobustScaler`
   - Encoded categorical variables using `OneHotEncoder`
   - Engineered date-based cyclic features (sine/cosine encoding)

2. **Feature Engineering**
   - Created new features such as:
     - Delivery days
     - Cyclic weekday/month encoding
     - Weekend indicators

3. **Model Training**
   - Models used:
     - Linear Regression  
     - Polynomial Regression  
     - Lasso Regression  
     - Ridge Regression  
     - Elastic Net Regression  
     - Random Forest Regressor  
     - AdaBoost Regressor  
     - XGBoost Regressor  

4. **Hyperparameter Tuning**
   - Used `GridSearchCV` for:
     - Ridge, Lasso, AdaBoost, XGBoost, ElasticNet, and RandomForest

---

## ğŸ§  Model Comparison

| Model                         | Leaderboard Score â†“ |
|-------------------------------|--------------------:|
| **Elastic Net (CV)**          | **3,985,281,371.543** |
| Elastic Net (Fixed)           | 4,523,090,717.491 |
| Linear (2 Features)           | 5,376,252,785.576 |
| AdaBoost (GridSearch)         | 6,240,594,182.929 |
| Ridge Regression (GridSearch) | 6,387,594,703.305 |
| Lasso Regression (GridSearch) | 6,510,598,453.306 |
| Ridge Regression (Fixed)      | 6,513,573,403.475 |
| Lasso Regression (Fixed)      | 6,515,160,094.766 |
| Linear Regression             | 6,515,164,682.050 |
| Polynomial Regression         | 9,309,575,885.102 |
| AdaBoost (Fixed)              | 11,090,862,349.089 |
| Random Forest (Fixed)         | 17,212,452,660.530 |
| XGBoost (GridSearch)          | 19,417,933,355.693 |
| Random Forest (GridSearch)    | 20,906,750,713.724 |
| XGBoost (Fixed)               | 24,642,928,711.251 |

> âœ… **Best Model:** Elastic Net Regression  
> It achieved the **lowest leaderboard score** and best generalization performance.

---

# ğŸ…‘ Multidimensional Personality Cluster Prediction  
**Assignment 2: Multinomial Classification**

## ğŸ¯ Objective
To predict an individual's **personality cluster (Aâ€“E)** based on behavioral and lifestyle attributes.

## ğŸ“Š Dataset
- `train.csv`: **1,913 Ã— 14**
- `test.csv`: **479 Ã— 13**
- **Target:** `personality_cluster`

## ğŸ§® Feature Categories
- **Numerical:** age group, upbringing influence, focus intensity, consistency score  
- **Binary:** identity code, external guidance usage  
- **Categorical:** cultural background (numerically encoded)

## ğŸ”¬ Feature Engineering
- Focus squared  
- Log focus  
- Focusâ€“consistency interaction  
- Activity strength  
- Stability mean  
- Guidance ratio  

## âš™ï¸ Preprocessing
- Scaling: Standard, Minâ€“Max, Robust  
- Encoding: One-Hot & Label Encoding  
- Missing Values: None  

## ğŸ“Š EDA Highlights
- Strong positive correlation between **consistency score and Cluster E**
- Heavy class overlap â†’ **non-linear decision boundaries**
- Most features show **low linear separability**

## ğŸ¤– Models Implemented
- Support Vector Machine (RBF Kernel)
- Multi-Layer Perceptron (MLP) âœ… **Best**
- Logistic Regression
- Naive Bayes
- Neural Network K-Fold
- Ensemble Models

## ğŸ† Best Model
- **MLP (256, 128, 64) with Label Encoding**
- **Best Leaderboard Score:** **0.627**

---
# ğŸ… Start-up Founder Retention Prediction  
**Assignment 2: Binomial Classification**

## ğŸ¯ Objective
To predict whether a **startup founder will stay with or leave** their startup based on personal, professional, and organizational attributes.

## ğŸ“Š Dataset
- `train.csv`: **59,611 Ã— 24**
- `test.csv`: **14,900 Ã— 23**
- **Target:** `retention_status`

## ğŸ” Key Features
- Founder age, gender  
- Years with startup  
- Monthly revenue generated  
- Work-life balance rating  
- Funding rounds led  
- Education background  
- Startup stage  
- Leadership scope  
- Startup reputation  
- Founder visibility  

## âš™ï¸ Data Preprocessing
- **Numerical Missing Values:** Filled using **Median**
- **Categorical Missing Values:** Filled using **Mode / "Unknown"**
- **Scaling:** StandardScaler
- **Encoding:** One-Hot Encoding

## ğŸ“Š Exploratory Data Analysis
- Boxplots for outlier detection  
- Histograms for feature distribution  
- Violin plots for group comparison  
- Correlation heat maps for numeric relationships  

### Key Insight:
> Numeric features alone show **weak separation** between retention classes â†’ retention is influenced by **multi-feature interactions**.

## ğŸ¤– Models Implemented
- Logistic Regression  
- Support Vector Machine (SVM)  
- K-Nearest Neighbors (KNN) âœ… **Best**
- Naive Bayes  
- XGBoost  
- Multi-Layer Perceptron (MLP)
- Stacking & Ensemble Methods  

## ğŸ† Best Model
- **K-Nearest Neighbors (KNN)**
- **Best Kaggle Score:** **0.749**
- Feature-scaled input with optimized neighbors
---

## ğŸ‘©â€ğŸ’» Team Members

Team Name: Project katakam
| Name | Roll No | Role |
|------|----------|------|
| **Katakam Shashidhar Sai** | IMT2023567 | Team Leader |
| Mohit Jagini | IMT2023528 | Member |
| Hardhik Dhavala | IMT2023579 | Member |

ğŸ“§ Team Lead Contact: [katakam.shashidhar@iiitb.ac.in](mailto:katakam.shashidhar@iiitb.ac.in)

---

## ğŸ“ References

1. Kaggle Challenge
      â€” [Medical Equipments Cost Prediction](https://www.kaggle.com/competitions/Medical-Equipments-Cost-Prediction-Challenge)
      - [Start-up Founder Retention Prediction](https://www.kaggle.com/competitions/start-up-founder-retention-prediction)
      - [Medical Equipments Cost Prediction](https://www.kaggle.com/competitions/multidimensional-personality-cluster-prediction)
3. Project Repository â€” [GitHub Link](https://github.com/DHardhik/ML-Challenge)

---

## ğŸ Final Output

The final model (`ElasticNetCV`) was saved and used to generate the submission file:

---
## ğŸ§© How to Run

1. Clone this repository:
   ```bash
   git clone https://github.com/yourusername/Medical-Cost-Prediction-Challenge.git
   cd Medical-Cost-Prediction-Challenge
2. Open chosen challenge Jupyter Notebook
3. Change the train and test data set file paths for Data Processing arc and Model Training Arc too.
